{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8cee0106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review :-\n",
      " hi this is my first review , gota love this feeling i love it\n",
      "Summary :-  great product  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "import traceback\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "import string\n",
    "import re\n",
    "import joblib\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Input,Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import models\n",
    "\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') \n",
    "\n",
    "nltk.data.path.append('./nltk_data/')\n",
    "\n",
    "latent_dim=500\n",
    "max_in_len=74\n",
    "max_tr_len=17\n",
    "\n",
    "\n",
    "stemm = LancasterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "contractions = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
    "                           \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
    "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
    "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
    "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
    "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
    "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
    "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
    "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
    "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
    "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
    "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
    "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
    "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
    "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
    "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
    "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
    "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
    "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
    "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
    "                           \"you're\": \"you are\", \"you've\": \"you have\"}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def clean_text(texts):\n",
    "    texts = BeautifulSoup(texts, \"lxml\").text   #remove the html tags\n",
    "    words=word_tokenize(texts.lower())  #tokenize the text into words \n",
    "    words= list(filter(lambda w:(w.isalpha() and len(w)>=3),words))\n",
    "    words= [contractions[w] if w in contractions else w for w in words ]\n",
    "    words= [stemm.stem(w) for w in words if w not in stop_words]\n",
    "    return words\n",
    "\n",
    "\n",
    "def helper(clean_text):\n",
    "    \n",
    "    # seq2seq model\n",
    "    model = models.load_model(\"./model/text_summarizer.h5\")\n",
    "    # text tokenizer\n",
    "    in_tokenizer = joblib.load('./model/text_tokenizer.pkl')\n",
    "    # summary tokenizer\n",
    "    tr_tokenizer = joblib.load('./model/summary_tokenizer.pkl')\n",
    "\n",
    "    \n",
    "    inp_x = in_tokenizer.texts_to_sequences([clean_text])\n",
    "    inp_x = pad_sequences(inp_x, maxlen=max_in_len, padding='post')\n",
    "    inp_x.reshape(1, max_in_len)\n",
    "    \n",
    "#     print(inp_x)\n",
    "\n",
    "    # encoder part\n",
    "    en_outputs,state_h_enc,state_c_enc = model.layers[6].output\n",
    "    en_states=[state_h_enc,state_c_enc]\n",
    "    en_model = Model(model.input[0],[en_outputs]+en_states)\n",
    "\n",
    "    # decoder inference\n",
    "    #create Input object for hidden and cell state for decoder shape of layer with hidden or latent dimension\n",
    "    dec_state_input_h = Input(shape=(latent_dim,))\n",
    "    dec_state_input_c = Input(shape=(latent_dim,))\n",
    "    dec_hidden_state_input = Input(shape=(max_in_len,latent_dim))\n",
    " \n",
    "    # Get the embeddings and input layer from the model\n",
    "    dec_inputs = model.input[1]\n",
    "    dec_emb_layer = model.layers[5]\n",
    "    dec_lstm = model.layers[7]\n",
    "    dec_embedding= dec_emb_layer(dec_inputs)\n",
    " \n",
    "    #add input and initialize LSTM layer with encoder LSTM states.\n",
    "    dec_outputs2, state_h2, state_c2 = dec_lstm(dec_embedding, initial_state=[dec_state_input_h,dec_state_input_c])\n",
    "\n",
    "    attention = model.layers[8]\n",
    "    attn_out2 = attention([dec_outputs2,dec_hidden_state_input])\n",
    " \n",
    "    merge2 = Concatenate(axis=-1)([dec_outputs2, attn_out2])\n",
    "\n",
    "    #Dense layer\n",
    "    dec_dense = model.layers[10]\n",
    "    dec_outputs3 = dec_dense(merge2)\n",
    " \n",
    "    # Finally define the Model Class\n",
    "    dec_model = Model(inputs=[dec_inputs] + [dec_hidden_state_input,dec_state_input_h,dec_state_input_c],outputs=[dec_outputs3] + [state_h2, state_c2])\n",
    "\n",
    "    #create a dictionary with a key as index and value as words.\n",
    "    reverse_target_word_index = tr_tokenizer.index_word\n",
    "    reverse_source_word_index = in_tokenizer.index_word\n",
    "    target_word_index = tr_tokenizer.word_index\n",
    "\n",
    "    # get the encoder output and states by passing the input sequence\n",
    "    en_out, en_h, en_c = en_model.predict(inp_x)\n",
    "\n",
    "    # target sequence with inital word as 'sos'\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    target_seq[0, 0] = target_word_index['sos']\n",
    "\n",
    "    # if the iteration reaches the end of text than it will be stop the iteration\n",
    "    stop_condition = False\n",
    "    # append every predicted word in decoded sentence\n",
    "    decoded_sentence = \"\"\n",
    "    while not stop_condition:\n",
    "        # get predicted output, hidden and cell state.\n",
    "        output_words, dec_h, dec_c = dec_model.predict([target_seq] + [en_out, en_h, en_c])\n",
    "\n",
    "        # get the index and from the dictionary get the word for that index.\n",
    "        word_index = np.argmax(output_words[0, -1, :])\n",
    "        text_word = reverse_target_word_index[word_index]\n",
    "        decoded_sentence += text_word + \" \"\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find a stop word or last word.\n",
    "        if text_word == \"eos\" or len(decoded_sentence) > max_tr_len:\n",
    "            stop_condition = True\n",
    "\n",
    "        # update target sequence to the current word index.\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = word_index\n",
    "        en_h, en_c = dec_h, dec_c\n",
    "\n",
    "    # return the deocded sentence\n",
    "    return decoded_sentence\n",
    "\n",
    "\n",
    "def predict(text):\n",
    "    try:\n",
    "        input_text = clean_text(text)\n",
    "        input_text = ' '.join(input_text)\n",
    "        \n",
    "        summary = helper(input_text)\n",
    "        if 'eos' in summary:\n",
    "            summary = summary.replace('eos', '')\n",
    "        return summary\n",
    "    except Exception:\n",
    "        print(traceback.print_exc())\n",
    "        return ''\n",
    "\n",
    "\n",
    "\n",
    "inp_review = \"\"\"hi this is my first review , gota love this feeling i love it\"\"\"\n",
    "print('Review :-\\n',inp_review)\n",
    "\n",
    "summary = predict(inp_review)\n",
    "print('Summary :- ',summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130ccd10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
